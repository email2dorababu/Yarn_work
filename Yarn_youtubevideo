https://www.youtube.com/watch?v=TIkm0x_s8bM
Installation of Apache Hadoop 2.6.0 as Single node on Ubuntu 14.10 (unicorn) Pseudo-distributed mode
Anindita's Blog

Step 1: sudo apt-get update
step 2: sudo apt-get install default-jdk
check java -version
step 3: sudo apt-get install ssh
step 4: sudo apt-get install rsync
step 5: ssh-keygen -t dsa -p '' -f ~/.ssh/id_dsa
step 6: cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
clear - we have now down with the configurations

Checking pwd less configuration.
ssh local

sudo tar -xvf hadoop-2.6.0
sudo mv hadoop-2.6.0 /usr/local/hadoop   # here we are moving to local directory.
check def path of jdk by,
update-alternatives --config java
usr/lib/javaxx/
copy this path and save it in notepad.

sudo vim .bashrc    #to set env setting
sudo apt-get install vim

sudo vim .bashrc

insert mode press I.
write the belo lines.
#Hadoop variables
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:HADOOP_HOME/bin
export PATH=$PATH:HADOOP_HOME/sbin
HADOOP_MAPRED_HOME=$HADOOP_HOME
HADOOP_COMMON_HOME=$HADOOP_HOME
HDADOOP_HDFS_HOOME=$HADOOP_HOME
YARN_HOME =$HADOOP_HOME
HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

shift :wq
then in the prompt to apply the settings
source  ~/.bashrc

goto hadoop folder
cd /usr/loal/hadoop

bin - standard service ,containers for hadoop hdfs, mapreduce,
etc - standard services needed , etc/hadoop bunch of xml that need to be configured.
include
lib
libexec
sbin - consists, services, all standeard shell scripts -stopping, balancer, distribute executions etc..
share

-----------------------------------
etc/hadoop 
first file that need to be changed is mapred-site.xml
mapred-site.xml.template is only will be there.
here we can directly work with the template.. but in older versions we need to go back and do some backup ..

write some standard configuration withing in xml.

property -name - mapreduce.framework.name , value yarn
move out from the folder.

open yarn-site.xml
property-name - yarn.nodemanager.aux-services value- mapreduce_shuffle
remember from 2.0 onwards, we need to write "_shuffle" , previous versions, we just need to wirte mapreduce.
for yarn services the value it is mapreduce_shuffle, to introduce shuffleing.

Now 
core-site.xml  - here we need to specifiy the name for hdfs , and we need to specifically define the name and port.
for single node it is local host and port is 9000

fs.default.name   hdfs://localhost:9000

----------------------
now hdfs-site.xml
couple of configurations..
property name - dfs.replication  value - 1
dfs.namenode.dir  - default location for namenode is file:///home/trainer/hadoopspace/hdfs/namenode
here we are going to save name node and data node directory.
dfs.data.dir - file:///home/trainer/hadoopspace/hdfs/namenode/datanode

next is environment settings file.
we need to specify the java home directory.

Next we need to create directory for namenode and datanode, so we need to create them.. the same path in hdfs-site.xml
mkdir -p /home/trainer/hadoopspace/hdfs/namenode
mkdir -p /home/trainer/hadoopspace/hdfs/namenode

sudo chown trainer:trainer -R /user/local/hadoop
the permission has been given to the current directory.

now format the namenode
hdfs namenode -format
to start the services
start-dfs.sh namenode secondary name node and data node.
start-yarn.sh  starts the yarn daemons nodemanager and resource manager.
run jps to see what are all the services are running.

check the version
hadoop version
you can check the webconsole

firefox http://localhost:50070
------------------------------------------------------------

https://www.youtube.com/watch?v=YY8QL25KCOg&spfreload=1

sudo apt-get install update
sudo apt-get install default-jdk
create a dedicated hadoop group and user
sudo addgroup hadoop
sudo adduser --ingroup hadoop hduser
lest add hduser as a sudoer or admin
sudo adduser hduser sudo
now lests install open ssh server
sudo apt-get install openssh-server openssh-client
now lets login with hduser and generate a key for hd user and add the key to the authorization keys
su hduser
cd
ssh-keyget -t rsa -P ""

cat $HOME/.ssh/id_rsa.pub>> $HOME/.ssh/autorixed_keys
let try by login to local host 
ssh localhost
if you see this message its all good
exit
clear
now let install hadoop 
follow the commands , or downlooad frpm apache
wget http://mirrors......../hadoop2.7.tar

tar xvzf 
now let move hadoop 2.7 to a directory of our choice in this tutorial we will choose /user/local/hadoop/
sudo mv hadoop-2.7  /usr/local/hadoop/

let give the directory to hduser as the owner
sudo chown -R hduser /usr/local
now let edit the bashrc file and append to the end of the file the path to hadoop

sudo nano ~/.bashrc

go to end
export Java_home
Hadoop_home
path =path+hadoop home bin
path = path/hadoophome/sbin
hadoop_mapred_home=hadoop hoem
hadoop common home= hadoop home
hadoop hdfs home = hadoop home
yarn home = hadoop home
hadoop common lib native dir = hadoop home /lib/native
hadoop opts="-Djava.library.path=$Hadoop_home/lib"
sorce ~/.bashrc
now let give the java path to run hadoop 
sudo nano /usr/local/hadoo/etc/hadoop/hadoop-env.sh

now let configure the following xml files
core-site, hdfs-site, yarn-site , mapred-site xmls

they are in etc/hadoop/ directory..







